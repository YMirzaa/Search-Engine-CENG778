{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from configFile import *\n",
    "from configFile import PartitioningType\n",
    "from utils import compareBM25List\n",
    "from utils import splitQueryAndTime\n",
    "from utils import Tokenizer\n",
    "import utils\n",
    "from node import Node\n",
    "import math\n",
    "\n",
    "docLengthsDictionary = utils.docLengthsDictionary\n",
    "docDic = utils.docDic\n",
    "\n",
    "### CONFIGS\n",
    "partitioningType = PartitioningType.TERMBASED\n",
    "partitionNum = 32\n",
    "\n",
    "wordListFileName = '../webData/wordlist.txt'\n",
    "queryListFileName = '../webData/10000.topics'\n",
    "### \n",
    "\n",
    "### GLOBALS\n",
    "cumulativeDic = {}\n",
    "documentDict = {}\n",
    "###\n",
    "#Reads the binary file and prints the entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def docLengths():\n",
    "    global docLengthsDictionary\n",
    "    with open('../webData/doc_lengths.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(' ')\n",
    "            docLengthsDictionary[line[0]] = line[1]\n",
    "            \n",
    "\n",
    "def readBinaryFile():\n",
    "    global docDic\n",
    "    k = 0\n",
    "    with open('../webData/entry.bin', 'rb') as f:\n",
    "        # data = f.seek(8*seekLength)\n",
    "        for (key,(startPos,pll)) in cumulativeDic.items():\n",
    "            data = (f.read(pll*8).hex(':', 4)).split(':')\n",
    "            # k+=1\n",
    "            # if(k==1000):\n",
    "            #     break\n",
    "            j = 0\n",
    "            docId = -1\n",
    "            docDic[key]= {}\n",
    "            for i in data:\n",
    "                newData = bytearray.fromhex(i)[::-1].hex(':',4)\n",
    "                if(j%2 == 0):\n",
    "                    docId = int(newData, 16)\n",
    "                else:\n",
    "                    docDic[key][docId] = int(newData, 16)\n",
    "                j+=1\n",
    "\n",
    "    # return docDic\n",
    "\n",
    "\n",
    "\n",
    "def wordListReader(filename):\n",
    "    wordList = []\n",
    "\n",
    "    with open(filename) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=' ')\n",
    "        for row in csv_reader:\n",
    "            wordList.append(row)\n",
    "    \n",
    "    return wordList\n",
    "\n",
    "\n",
    "def queryListReader(filename):\n",
    "    queryList = []\n",
    "\n",
    "    with open(filename, encoding='latin1') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=':')\n",
    "        for row in csv_reader:\n",
    "            querylistRow = row[1].translate(str.maketrans('','',string.punctuation))\n",
    "            queryList.append(querylistRow.split(' '))\n",
    "    \n",
    "    return queryList\n",
    "\n",
    "\n",
    "def termBasedPartitioner(wordList, partitionNum): \n",
    "    partitionDictList = []\n",
    "    for i in range(partitionNum):\n",
    "        newDict = dict()\n",
    "        partitionDictList.append(newDict)\n",
    "    termMap = {}\n",
    "    \n",
    "    cumulativeArr = [0]\n",
    "    global cumulativeDic\n",
    "    cumulativeDic = {}\n",
    "\n",
    "    listIndex = 0\n",
    "    while listIndex < len(wordList):\n",
    "        partitionIndex = listIndex % partitionNum\n",
    "        partitionDictList[partitionIndex][wordList[listIndex][0]] = wordList[listIndex][1]\n",
    "        termMap[wordList[listIndex][0]] = partitionIndex\n",
    "\n",
    "        cumulativeDic[wordList[listIndex][0]] = (cumulativeArr[-1], int(wordList[listIndex][1]))\n",
    "        cumulativeArr.append(cumulativeArr[-1]+int(wordList[listIndex][1]))\n",
    "\n",
    "        listIndex = listIndex + 1\n",
    "\n",
    "    return partitionDictList, termMap, cumulativeDic\n",
    "\n",
    "\n",
    "def pseudoPartition(partitionDict, query):\n",
    "    totalCost = 0\n",
    "    minCost = sys.maxsize\n",
    "\n",
    "    for term in query:\n",
    "        PLL = partitionDict.get(term)\n",
    "        if PLL is not None:\n",
    "            PLL = float(PLL)\n",
    "            if PLL > 0:\n",
    "                totalCost = totalCost + PLL\n",
    "                minCost = min(minCost, PLL)\n",
    "    \n",
    "    if minCost == sys.maxsize:\n",
    "        minCost = 0\n",
    "\n",
    "    return totalCost, minCost\n",
    "\n",
    "\n",
    "def broker(query, nodeList, termMap):\n",
    "    totalCost = 0\n",
    "    partitionCostList = []\n",
    "    totalPartitionCost = 0\n",
    "    minPartitionCost = sys.maxsize\n",
    "\n",
    "    nodeTermList = [[] for _ in range(len(nodeList))]\n",
    "\n",
    "    for term in query:\n",
    "        # NOTE: Check starting termMap Value index. 0 or 1\n",
    "        if term in termMap.keys():\n",
    "            nodeTermList[termMap[term]].append(term) \n",
    "\n",
    "\n",
    "    nodeTopKDocList = []\n",
    "    partitionIndex = 0\n",
    "    for node in nodeList:\n",
    "        if len(nodeTermList[partitionIndex]) != 0:\n",
    "            nodeTopKDoc = node.calculateTopKDoc(nodeTermList[partitionIndex])\n",
    "            nodeTopKDocList.append(nodeTopKDoc)\n",
    "            # print('nodeTopKDoc', nodeTopKDoc)\n",
    "\n",
    "        totalCost = totalCost + minPartitionCost\n",
    "        partitionCostList.append(totalPartitionCost)\n",
    "        partitionIndex = partitionIndex + 1\n",
    "    \n",
    "    topKDoc = compareBM25List(nodeTopKDocList)\n",
    "    \n",
    "    return topKDoc, totalCost, partitionCostList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList = wordListReader(wordListFileName)\n",
    "queryList, timeStamps = splitQueryAndTime()\n",
    "trainLength = math.floor(len(queryList)*0.70)\n",
    "queryListTest = queryList[trainLength:]\n",
    "queryListTrain = queryList[:trainLength]\n",
    "\n",
    "timeStampsTest = timeStamps[trainLength:]\n",
    "timeStampsTrain = timeStamps[:trainLength]\n",
    "\n",
    "nodeList = []\n",
    "global cumulativeDic\n",
    "if partitioningType.value == PartitioningType.TERMBASED.value:\n",
    "    partitioner = termBasedPartitioner\n",
    "\n",
    "[partitionDictList, termMap, cumulativeDic] = partitioner(wordList, partitionNum)\n",
    "docLengths()\n",
    "readBinaryFile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CACHE SIZE NUMBERS WILL BE HERE\n",
    "initialCacheSize = 40000 #In terms of PL length #5000 = 40kb\n",
    "maxCachePLLength = 400\n",
    "nodeList = []\n",
    "for nodeNum in range(partitionNum):\n",
    "    currentNode = Node(partitionDictList[nodeNum])\n",
    "    currentNode.EmptyCacheSize = initialCacheSize\n",
    "    nodeList.append(currentNode)\n",
    "\n",
    "queryIndex = 0\n",
    "brokerCost = 0\n",
    "partitioncCostList = len(partitionDictList) * [0] ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cacheFunctions\n",
    "import importlib\n",
    "importlib.reload(cacheFunctions)\n",
    "\n",
    "### NON-JUPYTER CODE\n",
    "# nodeFrequencyList = cacheFunctions.nodeFrequencyList \n",
    "# meansForIntervals = cacheFunctions.meansForIntervals\n",
    "# frequencyDict = cacheFunctions.frequencyDict\n",
    "\n",
    "# TEMP CLEARS\n",
    "cacheFunctions.nodeFrequencyList.clear()\n",
    "cacheFunctions.frequencyDict.clear()\n",
    "cacheFunctions.nodeFrequencyList = [[] for i in range(32)]\n",
    "\n",
    "termCaches = [{} for i in range(32)]\n",
    "\n",
    "cacheFunctions.frequencyDict = (cacheFunctions.queryFrequencies(queryListTrain))\n",
    "period = 16\n",
    "interval = 8\n",
    "# cacheFunctions.queryFrequencyStability(period, interval, queryListTrain, timeStampsTrain)\n",
    "\n",
    "cacheFunctions.everyNodeFrequency(termMap) # Frequency\n",
    "# cacheFunctions.stabilityFunction(termMap) # Stability\n",
    "\n",
    "# print(cacheFunctions.nodeFrequencyList)\n",
    "\n",
    "def cacheTermBM25():\n",
    "    lengthNFList = len(cacheFunctions.nodeFrequencyList)\n",
    "    for i in range(lengthNFList):\n",
    "        idx = 0\n",
    "        for term in cacheFunctions.nodeFrequencyList[i]:\n",
    "            cacheFunctions.nodeFrequencyList[i][idx][1][1] =  utils.BM25Algorithm(term[0])\n",
    "            cacheFunctions.nodeFrequencyList[i][idx][1][1].sort(key=lambda x: x[1], reverse=True)\n",
    "            idx+=1\n",
    "\n",
    "\n",
    "cacheTermBM25()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node,idx in zip(nodeList,range(len(nodeList))):\n",
    "    for cache in cacheFunctions.nodeFrequencyList[idx]:\n",
    "        length = len(cache[1][1])\n",
    "        if(length > maxCachePLLength):\n",
    "            length = maxCachePLLength\n",
    "\n",
    "        if(node.EmptyCacheSize >= length):        \n",
    "            node.Cache[cache[0]] = [cache[1][0],cache[1][1][:length]]\n",
    "            node.EmptyCacheSize -= length\n",
    "        elif(node.EmptyCacheSize > 0):\n",
    "            node.Cache[cache[0]] =  [cache[1][0],cache[1][1][:node.EmptyCacheSize]]\n",
    "            node.EmptyCacheSize = 0\n",
    "        else:\n",
    "            break\n",
    "    # print(len(node.Cache['jobs'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node id =>  0 , percentage hit => 0.30349099099099097 , cache hit counter=>  539 , total Recieved Term =>  1776\n",
      "Node id =>  1 , percentage hit => 0.27049754730203224 , cache hit counter=>  386 , total Recieved Term =>  1427\n",
      "Node id =>  2 , percentage hit => 0.37141249296567247 , cache hit counter=>  660 , total Recieved Term =>  1777\n",
      "Node id =>  3 , percentage hit => 0.30297029702970296 , cache hit counter=>  612 , total Recieved Term =>  2020\n",
      "Node id =>  4 , percentage hit => 0.5 , cache hit counter=>  1197 , total Recieved Term =>  2394\n",
      "Node id =>  5 , percentage hit => 0.26517083120856705 , cache hit counter=>  520 , total Recieved Term =>  1961\n",
      "Node id =>  6 , percentage hit => 0.3789858860428646 , cache hit counter=>  725 , total Recieved Term =>  1913\n",
      "Node id =>  7 , percentage hit => 0.27781013395457194 , cache hit counter=>  477 , total Recieved Term =>  1717\n",
      "Node id =>  8 , percentage hit => 0.26554621848739496 , cache hit counter=>  474 , total Recieved Term =>  1785\n",
      "Node id =>  9 , percentage hit => 0.3364485981308411 , cache hit counter=>  612 , total Recieved Term =>  1819\n",
      "Node id =>  10 , percentage hit => 0.2779740871613663 , cache hit counter=>  472 , total Recieved Term =>  1698\n",
      "Node id =>  11 , percentage hit => 0.3391647855530474 , cache hit counter=>  601 , total Recieved Term =>  1772\n",
      "Node id =>  12 , percentage hit => 0.3205051112447384 , cache hit counter=>  533 , total Recieved Term =>  1663\n",
      "Node id =>  13 , percentage hit => 0.44792671166827386 , cache hit counter=>  929 , total Recieved Term =>  2074\n",
      "Node id =>  14 , percentage hit => 0.2047589229805886 , cache hit counter=>  327 , total Recieved Term =>  1597\n",
      "Node id =>  15 , percentage hit => 0.3380202474690664 , cache hit counter=>  601 , total Recieved Term =>  1778\n",
      "Node id =>  16 , percentage hit => 0.34101941747572817 , cache hit counter=>  562 , total Recieved Term =>  1648\n",
      "Node id =>  17 , percentage hit => 0.4027849406910779 , cache hit counter=>  781 , total Recieved Term =>  1939\n",
      "Node id =>  18 , percentage hit => 0.4171270718232044 , cache hit counter=>  906 , total Recieved Term =>  2172\n",
      "Node id =>  19 , percentage hit => 0.3674469243331519 , cache hit counter=>  675 , total Recieved Term =>  1837\n",
      "Node id =>  20 , percentage hit => 0.3020176544766709 , cache hit counter=>  479 , total Recieved Term =>  1586\n",
      "Node id =>  21 , percentage hit => 0.27784810126582277 , cache hit counter=>  439 , total Recieved Term =>  1580\n",
      "Node id =>  22 , percentage hit => 0.3106283029947152 , cache hit counter=>  529 , total Recieved Term =>  1703\n",
      "Node id =>  23 , percentage hit => 0.3683223992502343 , cache hit counter=>  786 , total Recieved Term =>  2134\n",
      "Node id =>  24 , percentage hit => 0.23282442748091603 , cache hit counter=>  366 , total Recieved Term =>  1572\n",
      "Node id =>  25 , percentage hit => 0.3494199535962877 , cache hit counter=>  753 , total Recieved Term =>  2155\n",
      "Node id =>  26 , percentage hit => 0.36576576576576575 , cache hit counter=>  609 , total Recieved Term =>  1665\n",
      "Node id =>  27 , percentage hit => 0.3533201189296333 , cache hit counter=>  713 , total Recieved Term =>  2018\n",
      "Node id =>  28 , percentage hit => 0.44449520328917314 , cache hit counter=>  973 , total Recieved Term =>  2189\n",
      "Node id =>  29 , percentage hit => 0.25246753246753245 , cache hit counter=>  486 , total Recieved Term =>  1925\n",
      "Node id =>  30 , percentage hit => 0.26456477039067855 , cache hit counter=>  386 , total Recieved Term =>  1459\n",
      "Node id =>  31 , percentage hit => 0.25662154208357857 , cache hit counter=>  436 , total Recieved Term =>  1699\n"
     ]
    }
   ],
   "source": [
    "for query in queryListTest:\n",
    "    \n",
    "   #  print('before tokenizer',query)\n",
    "    newQuery = []\n",
    "    newQuery = Tokenizer.tokenize(query)\n",
    "   #  print('after tokenizer= =', newQuery)\n",
    "    [topKDoc, currBrokerCost, currPartitionCostList] = broker(newQuery, nodeList, termMap)\n",
    "    brokerCost = brokerCost + currBrokerCost\n",
    "    partitioncCostList = list(map(add, partitioncCostList, currPartitionCostList))\n",
    "    queryIndex = queryIndex + 1\n",
    "   #  print('topKDoc',topKDoc[:200])\n",
    "    \n",
    "   #  sys.exit(0)\n",
    "for node,idx in zip(nodeList,range(len(nodeList))):\n",
    "   print('Node id => ',idx, ', percentage hit =>', node.cacheHitCounter/node.totalRecievedTerm , ', cache hit counter=> ', node.cacheHitCounter, ', total Recieved Term => ' , node.totalRecievedTerm)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
